# LiteLLM Configuration (Primary)
LITELLM_BASE_URL=http://localhost:4000
LITELLM_API_KEY=your_litellm_api_key_here

# Model Configuration
# Model for text generation (descriptions) - via LiteLLM proxy
LLM_MODEL=gpt-4o
# Model for vision/image parsing (menu image analysis) - requires multimodal capability
# Examples: gpt-4o, gemini-2.0-flash, claude-3-5-sonnet
VISION_MODEL=gpt-4o
# Model for Image Generation (via LiteLLM proxy)
# Examples: dall-e-3, gemini-3-pro-image-preview
IMAGE_GEN_MODEL=gemini-3-pro-image-preview
# Model for Video Generation (Google Veo via LiteLLM proxy)
# Options: veo-3.0-generate-001, veo-3.1-generate-001
VIDEO_GEN_MODEL=veo-3.0-generate-001

# Image Provider Selection
# Options: "litellm" (uses IMAGE_GEN_MODEL via LiteLLM/Proxy) or "nvidia" (uses NVIDIA_IMAGE_GEN_URL directly)
IMAGE_PROVIDER=litellm

# NVIDIA Configuration (for direct NVIDIA API usage)
# Note: The backend automatically falls back from large to medium model if 404 is returned,
# so you can use either URL. The medium model is more widely available.
NVIDIA_API_KEY=your_nvidia_api_key_here
NVIDIA_IMAGE_GEN_URL=https://ai.api.nvidia.com/v1/genai/stabilityai/stable-diffusion-3-medium

# Server Configuration
LOG_LEVEL=INFO
